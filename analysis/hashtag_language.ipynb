{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0c4c1a3e9c3a39a0885e9d9bd6e4723f2ede522dfea9ed13682022cc292aed1ac",
   "display_name": "Python 3.7.10 64-bit ('dask-cudf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse as sps\n",
    "import pandas as pd\n",
    "\n",
    "from utils.preprocessing import *\n",
    "from utils.util import chainer\n",
    "from utils.target_encode import MTE_one_shot\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import core.config as conf"
   ]
  },
  {
   "source": [
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = conf.raw_lzo_path + 'part-00000'\n",
    "ori_df = read_data(data_path)\n",
    "df = ori_df[['hashtags', 'tweet_type', 'language', 'tweet_id', 'reply_timestamp', 'retweet_timestamp', 'comment_timestamp', 'like_timestamp']].copy()\n",
    "df = df.dropna(subset=['hashtags'])\n",
    "\n",
    "print('total exampls with hastags: ', len(df['hashtags'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags_list'] = df['hashtags'].str.split('\\t')\n",
    "df['hashtags_cnt'] = df['hashtags'].str.count('\\t')\n",
    "df['hashtags_cnt'] = df['hashtags_cnt'].astype(int) + 1\n",
    "\n",
    "print('max hashtag count: ', df['hashtags_cnt'].max())\n",
    "print('min hashtag count: ', df['hashtags_cnt'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'id': np.repeat(df.index.to_series(), df['hashtags_cnt']),\n",
    "    'tweet_type': np.repeat(df['tweet_type'], df['hashtags_cnt']),\n",
    "    'language': np.repeat(df['language'], df['hashtags_cnt']),\n",
    "    'retweet_timestamp': np.repeat(df['retweet_timestamp'], df['hashtags_cnt']),\n",
    "    'comment_timestamp': np.repeat(df['comment_timestamp'], df['hashtags_cnt']),\n",
    "    'like_timestamp': np.repeat(df['like_timestamp'], df['hashtags_cnt']),\n",
    "    'reply_timestamp': np.repeat(df['reply_timestamp'], df['hashtags_cnt']),\n",
    "    'hashtags': chainer(df['hashtags']) # row로 나누기\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "source": [
    "## Encode hashtags & langauge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language encode\n",
    "\n",
    "langauge_df = read_data('../data/language.csv', sep=',', features=['language_id', 'language', 'language_type']).reset_index(drop=True)\n",
    "\n",
    "language_to_idx = dict(zip(langauge_df['language'], langauge_df['language_id']))\n",
    "idx_to_language = dict(zip(langauge_df['language_id'], langauge_df['language']))\n",
    "\n",
    "df['language_encode'] = df['language'].apply(lambda x: language_to_idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling\n",
    "for label in (conf.labels):\n",
    "    label_name = label.split('_')[0]\n",
    "    df.loc[df[label]<=0, label_name ] = 0\n",
    "    df.loc[df[label]>0, label_name ] = 1\n",
    "    df = df.drop([label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag encode\n",
    "var = df['hashtags'].fillna('').values.copy()\n",
    "gc.collect()\n",
    "\n",
    "PD = {}\n",
    "null = var[0]\n",
    "PD[null] = [0,0]\n",
    "count = 1\n",
    "for v in var:\n",
    "    if v not in PD:\n",
    "        PD[v] = [count,1]\n",
    "        count +=1\n",
    "    else:\n",
    "        x = PD[v]\n",
    "        x[1] += 1\n",
    "        PD[v] = x\n",
    "\n",
    "vari = []\n",
    "for v in var:\n",
    "    li=[]\n",
    "    lf=[]\n",
    "    if v!='':\n",
    "        li.append(PD[v][0])\n",
    "        lf.append(-PD[v][1])\n",
    "    vari.append( list(np.array(li)[np.argsort(lf)].astype(np.int32) ) )\n",
    "    \n",
    "del PD\n",
    "gc.collect()\n",
    "\n",
    "len(vari), vari[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags_encode'] = np.array( [v[0] for v in vari ] ).astype( np.int32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rows: ', len(df))\n",
    "print('total unique hashtags: ', len(df['hashtags'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_cnt = df['language_encode'].value_counts().sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cnt = df['hashtags'].value_counts().sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_types = langauge_df.language_id.to_list()\n",
    "n_languages = len(language_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring some auxiliary structures\n",
    "n_engagements_arr = np.zeros(n_languages).astype('int32')\n",
    "n_positive_engagements_arr = np.empty(n_languages).astype('int32')\n",
    "n_negative_engagements_arr = np.empty(n_languages).astype('int32')\n",
    "n_like_engagements_arr = np.empty(n_languages).astype('int32')\n",
    "n_retweet_engagements_arr = np.empty(n_languages).astype('int32')\n",
    "n_reply_engagements_arr = np.empty(n_languages) .astype('int32')\n",
    "n_comment_engagements_arr = np.empty(n_languages).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_result = df.groupby(['language_encode', 'hashtags']).sum()\n",
    "aggregate_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_result = df.groupby(['language_encode', 'hashtags']).count()\n",
    "aggregate_result = aggregate_result[[\"id\"]]\n",
    "aggregate_result['cnt'] = aggregate_result['id'] \n",
    "aggregate_result = aggregate_result.drop('id', axis=1)\n",
    "aggregate_result = aggregate_result.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aggregate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 100\n",
    "language_hashtags = [[] for _ in range(n_languages)]\n",
    "for i in range(n_languages):\n",
    "    try:\n",
    "        tmp = aggregate_result.loc[aggregate_result['language_encode'] == i]\n",
    "        tmp = tmp.sort_values('cnt', ascending=False)[:top_n]\n",
    "\n",
    "        language_hashtags[i] = tmp\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_hashtags[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_hashtags[0]['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_languages)[:5]:\n",
    "    plt.bar(range(len(language_hashtags[i])), language_hashtags[i]['cnt'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_languages):\n",
    "    if len(language_hashtags[i]['cnt']) == 0:\n",
    "        print(f\"Language {i} - max tag count: 0 \")\n",
    "    else:\n",
    "        max_cnt = language_hashtags[i]['cnt'].max()\n",
    "        print(f\"Language {i} - max tag count: {int(max_cnt):n} \")"
   ]
  },
  {
   "source": [
    "## Anlaysis hashtag count for all data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language encoder\n",
    "language_df = read_data('../data/language.csv', sep=',', features=['language_id', 'language', 'language_type']).reset_index(drop=True)\n",
    "\n",
    "language_to_idx = dict(zip(language_df['language'], language_df['language_id']))\n",
    "idx_to_language = dict(zip(language_df['language_id'], language_df['language']))\n",
    "\n",
    "language_types = language_df.language_id.to_list()\n",
    "n_languages = len(language_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/it]total exampls with hastags:  part-00094  :  596527\n",
      " 33%|███▎      | 95/291 [33:12<1:10:34, 21.61s/it]total exampls with hastags:  part-00095  :  596073\n",
      " 33%|███▎      | 96/291 [33:34<1:10:10, 21.59s/it]total exampls with hastags:  part-00096  :  598176\n",
      " 33%|███▎      | 97/291 [33:56<1:09:49, 21.60s/it]total exampls with hastags:  part-00097  :  597368\n",
      " 34%|███▎      | 98/291 [34:17<1:09:25, 21.58s/it]total exampls with hastags:  part-00098  :  596011\n",
      " 34%|███▍      | 99/291 [34:38<1:08:34, 21.43s/it]total exampls with hastags:  part-00099  :  596563\n",
      " 34%|███▍      | 100/291 [34:59<1:08:04, 21.38s/it]total exampls with hastags:  part-00100  :  596537\n",
      " 35%|███▍      | 101/291 [35:21<1:07:24, 21.29s/it]total exampls with hastags:  part-00101  :  595904\n",
      " 35%|███▌      | 102/291 [35:42<1:06:50, 21.22s/it]total exampls with hastags:  part-00102  :  597013\n",
      " 35%|███▌      | 103/291 [36:03<1:06:23, 21.19s/it]total exampls with hastags:  part-00103  :  598051\n",
      " 36%|███▌      | 104/291 [36:24<1:06:03, 21.19s/it]total exampls with hastags:  part-00104  :  596490\n",
      " 36%|███▌      | 105/291 [36:45<1:05:50, 21.24s/it]total exampls with hastags:  part-00105  :  596710\n",
      " 36%|███▋      | 106/291 [37:07<1:05:40, 21.30s/it]total exampls with hastags:  part-00106  :  596877\n",
      " 37%|███▋      | 107/291 [37:28<1:05:08, 21.24s/it]total exampls with hastags:  part-00107  :  597797\n",
      " 37%|███▋      | 108/291 [37:49<1:04:58, 21.30s/it]total exampls with hastags:  part-00108  :  595853\n",
      " 37%|███▋      | 109/291 [38:11<1:04:54, 21.40s/it]total exampls with hastags:  part-00109  :  597298\n",
      " 38%|███▊      | 110/291 [38:32<1:04:28, 21.37s/it]total exampls with hastags:  part-00110  :  596029\n",
      " 38%|███▊      | 111/291 [38:54<1:04:17, 21.43s/it]total exampls with hastags:  part-00111  :  596810\n",
      " 38%|███▊      | 112/291 [39:15<1:03:38, 21.33s/it]total exampls with hastags:  part-00112  :  597920\n",
      " 39%|███▉      | 113/291 [39:36<1:02:55, 21.21s/it]total exampls with hastags:  part-00113  :  596273\n",
      " 39%|███▉      | 114/291 [39:57<1:02:56, 21.33s/it]total exampls with hastags:  part-00114  :  595111\n",
      " 40%|███▉      | 115/291 [40:19<1:02:47, 21.41s/it]total exampls with hastags:  part-00115  :  596430\n",
      " 40%|███▉      | 116/291 [40:40<1:02:26, 21.41s/it]total exampls with hastags:  part-00116  :  597418\n",
      " 40%|████      | 117/291 [41:02<1:02:21, 21.50s/it]total exampls with hastags:  part-00117  :  595990\n",
      " 41%|████      | 118/291 [41:24<1:01:59, 21.50s/it]total exampls with hastags:  part-00118  :  596298\n",
      " 41%|████      | 119/291 [41:45<1:01:35, 21.49s/it]total exampls with hastags:  part-00119  :  596527\n",
      " 41%|████      | 120/291 [42:07<1:01:19, 21.51s/it]total exampls with hastags:  part-00120  :  595825\n",
      " 42%|████▏     | 121/291 [42:28<1:00:59, 21.53s/it]total exampls with hastags:  part-00121  :  597357\n",
      " 42%|████▏     | 122/291 [42:50<1:00:51, 21.61s/it]total exampls with hastags:  part-00122  :  596407\n",
      " 42%|████▏     | 123/291 [43:11<59:56, 21.41s/it]  total exampls with hastags:  part-00123  :  596749\n",
      " 43%|████▎     | 124/291 [43:32<59:24, 21.34s/it]total exampls with hastags:  part-00124  :  596953\n",
      " 43%|████▎     | 125/291 [43:54<59:12, 21.40s/it]total exampls with hastags:  part-00125  :  596546\n",
      " 43%|████▎     | 126/291 [44:15<59:03, 21.48s/it]total exampls with hastags:  part-00126  :  596166\n",
      " 44%|████▎     | 127/291 [44:37<58:43, 21.48s/it]total exampls with hastags:  part-00127  :  597345\n",
      " 44%|████▍     | 128/291 [44:58<58:28, 21.52s/it]total exampls with hastags:  part-00128  :  597295\n",
      " 44%|████▍     | 129/291 [45:20<58:24, 21.63s/it]total exampls with hastags:  part-00129  :  597414\n",
      " 45%|████▍     | 130/291 [45:42<57:46, 21.53s/it]total exampls with hastags:  part-00130  :  596832\n",
      " 45%|████▌     | 131/291 [46:03<57:22, 21.52s/it]total exampls with hastags:  part-00131  :  596697\n",
      " 45%|████▌     | 132/291 [46:25<57:06, 21.55s/it]total exampls with hastags:  part-00132  :  596226\n",
      " 46%|████▌     | 133/291 [46:45<55:54, 21.23s/it]total exampls with hastags:  part-00133  :  596701\n",
      " 46%|████▌     | 134/291 [47:06<55:05, 21.05s/it]total exampls with hastags:  part-00134  :  597773\n",
      " 46%|████▋     | 135/291 [47:26<54:20, 20.90s/it]total exampls with hastags:  part-00135  :  597289\n",
      " 47%|████▋     | 136/291 [47:47<54:04, 20.93s/it]total exampls with hastags:  part-00136  :  598269\n",
      " 47%|████▋     | 137/291 [48:08<53:38, 20.90s/it]total exampls with hastags:  part-00137  :  596931\n",
      " 47%|████▋     | 138/291 [48:30<53:38, 21.04s/it]total exampls with hastags:  part-00138  :  595832\n",
      " 48%|████▊     | 139/291 [48:51<53:29, 21.11s/it]total exampls with hastags:  part-00139  :  595204\n",
      " 48%|████▊     | 140/291 [49:12<53:15, 21.16s/it]total exampls with hastags:  part-00140  :  596164\n",
      " 48%|████▊     | 141/291 [49:33<52:57, 21.18s/it]total exampls with hastags:  part-00141  :  597281\n",
      " 49%|████▉     | 142/291 [49:55<53:05, 21.38s/it]total exampls with hastags:  part-00142  :  596580\n",
      " 49%|████▉     | 143/291 [50:17<52:50, 21.42s/it]total exampls with hastags:  part-00143  :  595925\n",
      " 49%|████▉     | 144/291 [50:38<52:39, 21.49s/it]total exampls with hastags:  part-00144  :  597383\n",
      " 50%|████▉     | 145/291 [51:00<52:20, 21.51s/it]total exampls with hastags:  part-00145  :  596751\n",
      " 50%|█████     | 146/291 [51:21<51:56, 21.49s/it]total exampls with hastags:  part-00146  :  596425\n",
      " 51%|█████     | 147/291 [51:43<51:22, 21.40s/it]total exampls with hastags:  part-00147  :  597161\n",
      " 51%|█████     | 148/291 [52:04<51:02, 21.41s/it]total exampls with hastags:  part-00148  :  596179\n",
      " 51%|█████     | 149/291 [52:26<50:50, 21.48s/it]total exampls with hastags:  part-00149  :  596529\n",
      " 52%|█████▏    | 150/291 [52:47<50:33, 21.51s/it]total exampls with hastags:  part-00150  :  596488\n",
      " 52%|█████▏    | 151/291 [53:09<50:11, 21.51s/it]total exampls with hastags:  part-00151  :  595837\n",
      " 52%|█████▏    | 152/291 [53:30<49:53, 21.54s/it]total exampls with hastags:  part-00152  :  597740\n",
      " 53%|█████▎    | 153/291 [53:52<49:30, 21.53s/it]total exampls with hastags:  part-00153  :  596246\n",
      " 53%|█████▎    | 154/291 [54:13<49:00, 21.46s/it]total exampls with hastags:  part-00154  :  597245\n",
      " 53%|█████▎    | 155/291 [54:35<48:51, 21.55s/it]total exampls with hastags:  part-00155  :  596681\n",
      " 54%|█████▎    | 156/291 [54:57<48:34, 21.59s/it]total exampls with hastags:  part-00156  :  597187\n",
      " 54%|█████▍    | 157/291 [55:18<48:10, 21.57s/it]total exampls with hastags:  part-00157  :  597669\n",
      " 54%|█████▍    | 158/291 [55:40<47:41, 21.51s/it]total exampls with hastags:  part-00158  :  597133\n",
      " 55%|█████▍    | 159/291 [56:01<47:10, 21.44s/it]total exampls with hastags:  part-00159  :  597267\n",
      " 55%|█████▍    | 160/291 [56:22<46:48, 21.44s/it]total exampls with hastags:  part-00160  :  596184\n",
      " 55%|█████▌    | 161/291 [56:44<46:34, 21.49s/it]total exampls with hastags:  part-00161  :  596232\n",
      " 56%|█████▌    | 162/291 [57:05<46:04, 21.43s/it]total exampls with hastags:  part-00162  :  597103\n",
      " 56%|█████▌    | 163/291 [57:26<45:33, 21.35s/it]total exampls with hastags:  part-00163  :  597013\n",
      " 56%|█████▋    | 164/291 [57:48<45:24, 21.45s/it]total exampls with hastags:  part-00164  :  596281\n",
      " 57%|█████▋    | 165/291 [58:10<45:18, 21.57s/it]total exampls with hastags:  part-00165  :  597097\n",
      " 57%|█████▋    | 166/291 [58:31<44:57, 21.58s/it]total exampls with hastags:  part-00166  :  597078\n",
      " 57%|█████▋    | 167/291 [58:53<44:26, 21.50s/it]total exampls with hastags:  part-00167  :  596762\n",
      " 58%|█████▊    | 168/291 [59:14<43:54, 21.42s/it]total exampls with hastags:  part-00168  :  596494\n",
      " 58%|█████▊    | 169/291 [59:35<43:27, 21.37s/it]total exampls with hastags:  part-00169  :  596008\n",
      " 58%|█████▊    | 170/291 [59:57<43:01, 21.33s/it]total exampls with hastags:  part-00170  :  595954\n",
      " 59%|█████▉    | 171/291 [1:00:18<42:49, 21.41s/it]total exampls with hastags:  part-00171  :  596972\n",
      " 59%|█████▉    | 172/291 [1:00:39<42:21, 21.35s/it]total exampls with hastags:  part-00172  :  596849\n",
      " 59%|█████▉    | 173/291 [1:01:01<42:04, 21.40s/it]total exampls with hastags:  part-00173  :  596818\n",
      " 60%|█████▉    | 174/291 [1:01:22<41:48, 21.44s/it]total exampls with hastags:  part-00174  :  596935\n",
      " 60%|██████    | 175/291 [1:01:44<41:31, 21.48s/it]total exampls with hastags:  part-00175  :  597297\n",
      " 60%|██████    | 176/291 [1:02:06<41:33, 21.68s/it]total exampls with hastags:  part-00176  :  596769\n",
      " 61%|██████    | 177/291 [1:02:28<41:11, 21.68s/it]total exampls with hastags:  part-00177  :  597586\n",
      " 61%|██████    | 178/291 [1:02:49<40:50, 21.68s/it]total exampls with hastags:  part-00178  :  595637\n",
      " 62%|██████▏   | 179/291 [1:03:11<40:17, 21.59s/it]total exampls with hastags:  part-00179  :  596663\n",
      " 62%|██████▏   | 180/291 [1:03:32<39:59, 21.61s/it]total exampls with hastags:  part-00180  :  597026\n",
      " 62%|██████▏   | 181/291 [1:03:54<39:30, 21.55s/it]total exampls with hastags:  part-00181  :  597201\n",
      " 63%|██████▎   | 182/291 [1:04:15<39:01, 21.48s/it]total exampls with hastags:  part-00182  :  596876\n",
      " 63%|██████▎   | 183/291 [1:04:37<38:41, 21.50s/it]total exampls with hastags:  part-00183  :  596786\n",
      " 63%|██████▎   | 184/291 [1:04:59<38:34, 21.63s/it]total exampls with hastags:  part-00184  :  596441\n",
      " 64%|██████▎   | 185/291 [1:05:20<38:09, 21.60s/it]total exampls with hastags:  part-00185  :  597190\n",
      " 64%|██████▍   | 186/291 [1:05:42<37:47, 21.60s/it]total exampls with hastags:  part-00186  :  597188\n",
      " 64%|██████▍   | 187/291 [1:06:03<37:18, 21.53s/it]total exampls with hastags:  part-00187  :  596565\n",
      " 65%|██████▍   | 188/291 [1:06:24<36:50, 21.46s/it]total exampls with hastags:  part-00188  :  597027\n",
      " 65%|██████▍   | 189/291 [1:06:46<36:21, 21.39s/it]total exampls with hastags:  part-00189  :  595972\n",
      " 65%|██████▌   | 190/291 [1:07:07<35:46, 21.26s/it]total exampls with hastags:  part-00190  :  595734\n",
      " 66%|██████▌   | 191/291 [1:07:29<35:53, 21.54s/it]total exampls with hastags:  part-00191  :  596965\n",
      " 66%|██████▌   | 192/291 [1:07:51<35:53, 21.75s/it]total exampls with hastags:  part-00192  :  597386\n",
      " 66%|██████▋   | 193/291 [1:08:12<35:18, 21.62s/it]total exampls with hastags:  part-00193  :  595489\n",
      " 67%|██████▋   | 194/291 [1:08:34<35:06, 21.72s/it]total exampls with hastags:  part-00194  :  595833\n",
      " 67%|██████▋   | 195/291 [1:08:55<34:27, 21.54s/it]total exampls with hastags:  part-00195  :  596043\n",
      " 67%|██████▋   | 196/291 [1:09:17<33:57, 21.44s/it]total exampls with hastags:  part-00196  :  596386\n",
      " 68%|██████▊   | 197/291 [1:09:38<33:28, 21.37s/it]total exampls with hastags:  part-00197  :  596005\n",
      " 68%|██████▊   | 198/291 [1:09:59<33:01, 21.31s/it]total exampls with hastags:  part-00198  :  597232\n",
      " 68%|██████▊   | 199/291 [1:10:20<32:43, 21.34s/it]total exampls with hastags:  part-00199  :  596053\n",
      " 69%|██████▊   | 200/291 [1:10:42<32:19, 21.32s/it]total exampls with hastags:  part-00200  :  594923\n",
      " 69%|██████▉   | 201/291 [1:11:03<32:00, 21.34s/it]total exampls with hastags:  part-00201  :  586916\n",
      " 69%|██████▉   | 202/291 [1:11:24<31:28, 21.21s/it]total exampls with hastags:  part-00202  :  557105\n",
      " 70%|██████▉   | 203/291 [1:11:44<30:38, 20.89s/it]total exampls with hastags:  part-00203  :  544306\n",
      " 70%|███████   | 204/291 [1:12:04<29:40, 20.46s/it]total exampls with hastags:  part-00204  :  544572\n",
      " 70%|███████   | 205/291 [1:12:23<28:58, 20.22s/it]total exampls with hastags:  part-00205  :  537573\n",
      " 71%|███████   | 206/291 [1:12:43<28:15, 19.95s/it]total exampls with hastags:  part-00206  :  537147\n",
      " 71%|███████   | 207/291 [1:13:02<27:36, 19.72s/it]total exampls with hastags:  part-00207  :  533196\n",
      " 71%|███████▏  | 208/291 [1:13:21<27:00, 19.53s/it]total exampls with hastags:  part-00208  :  524035\n",
      " 72%|███████▏  | 209/291 [1:13:39<26:18, 19.25s/it]total exampls with hastags:  part-00209  :  513797\n",
      " 72%|███████▏  | 210/291 [1:13:58<25:30, 18.90s/it]total exampls with hastags:  part-00210  :  497324\n",
      " 73%|███████▎  | 211/291 [1:14:15<24:44, 18.56s/it]total exampls with hastags:  part-00211  :  493201\n",
      " 73%|███████▎  | 212/291 [1:14:33<24:09, 18.35s/it]total exampls with hastags:  part-00212  :  493542\n",
      " 73%|███████▎  | 213/291 [1:14:51<23:27, 18.05s/it]total exampls with hastags:  part-00213  :  493448\n",
      " 74%|███████▎  | 214/291 [1:15:08<23:03, 17.96s/it]total exampls with hastags:  part-00214  :  489434\n",
      " 74%|███████▍  | 215/291 [1:15:26<22:33, 17.81s/it]total exampls with hastags:  part-00215  :  470033\n",
      " 74%|███████▍  | 216/291 [1:15:43<21:58, 17.58s/it]total exampls with hastags:  part-00216  :  469474\n",
      " 75%|███████▍  | 217/291 [1:16:00<21:24, 17.36s/it]total exampls with hastags:  part-00217  :  425022\n",
      " 75%|███████▍  | 218/291 [1:16:15<20:28, 16.84s/it]total exampls with hastags:  part-00218  :  421213\n",
      " 75%|███████▌  | 219/291 [1:16:30<19:37, 16.35s/it]total exampls with hastags:  part-00219  :  408702\n",
      " 76%|███████▌  | 220/291 [1:16:46<18:52, 15.96s/it]total exampls with hastags:  part-00220  :  403920\n",
      " 76%|███████▌  | 221/291 [1:17:00<18:12, 15.60s/it]total exampls with hastags:  part-00221  :  398565\n",
      " 76%|███████▋  | 222/291 [1:17:15<17:33, 15.27s/it]total exampls with hastags:  part-00222  :  395764\n",
      " 77%|███████▋  | 223/291 [1:17:29<16:58, 14.97s/it]total exampls with hastags:  part-00223  :  393898\n",
      " 77%|███████▋  | 224/291 [1:17:43<16:29, 14.76s/it]total exampls with hastags:  part-00224  :  394093\n",
      " 77%|███████▋  | 225/291 [1:17:59<16:29, 14.99s/it]total exampls with hastags:  part-00225  :  393259\n",
      " 78%|███████▊  | 226/291 [1:18:14<16:13, 14.98s/it]total exampls with hastags:  part-00226  :  391244\n",
      " 78%|███████▊  | 227/291 [1:18:28<15:49, 14.84s/it]total exampls with hastags:  part-00227  :  386487\n",
      " 78%|███████▊  | 228/291 [1:18:42<15:19, 14.60s/it]total exampls with hastags:  part-00228  :  383027\n",
      " 79%|███████▊  | 229/291 [1:18:56<14:52, 14.39s/it]total exampls with hastags:  part-00229  :  383560\n",
      " 79%|███████▉  | 230/291 [1:19:10<14:28, 14.23s/it]total exampls with hastags:  part-00230  :  381813\n",
      " 79%|███████▉  | 231/291 [1:19:24<14:09, 14.15s/it]total exampls with hastags:  part-00231  :  382072\n",
      " 80%|███████▉  | 232/291 [1:19:38<13:49, 14.06s/it]total exampls with hastags:  part-00232  :  379073\n",
      " 80%|████████  | 233/291 [1:19:52<13:42, 14.19s/it]total exampls with hastags:  part-00233  :  378886\n",
      " 80%|████████  | 234/291 [1:20:06<13:21, 14.07s/it]total exampls with hastags:  part-00234  :  377228\n",
      " 81%|████████  | 235/291 [1:20:20<13:02, 13.97s/it]total exampls with hastags:  part-00235  :  376187\n",
      " 81%|████████  | 236/291 [1:20:34<12:48, 13.97s/it]total exampls with hastags:  part-00236  :  375748\n",
      " 81%|████████▏ | 237/291 [1:20:48<12:42, 14.12s/it]total exampls with hastags:  part-00237  :  374225\n",
      " 82%|████████▏ | 238/291 [1:21:02<12:22, 14.00s/it]total exampls with hastags:  part-00238  :  374469\n",
      " 82%|████████▏ | 239/291 [1:21:16<12:07, 13.99s/it]total exampls with hastags:  part-00239  :  374627\n",
      " 82%|████████▏ | 240/291 [1:21:30<11:52, 13.97s/it]total exampls with hastags:  part-00240  :  372996\n",
      " 83%|████████▎ | 241/291 [1:21:43<11:29, 13.79s/it]total exampls with hastags:  part-00241  :  371681\n",
      " 83%|████████▎ | 242/291 [1:21:57<11:10, 13.68s/it]total exampls with hastags:  part-00242  :  370876\n",
      " 84%|████████▎ | 243/291 [1:22:10<10:55, 13.65s/it]total exampls with hastags:  part-00243  :  368086\n",
      " 84%|████████▍ | 244/291 [1:22:24<10:38, 13.59s/it]total exampls with hastags:  part-00244  :  368306\n",
      " 84%|████████▍ | 245/291 [1:22:37<10:24, 13.57s/it]total exampls with hastags:  part-00245  :  368619\n",
      " 85%|████████▍ | 246/291 [1:22:51<10:09, 13.54s/it]total exampls with hastags:  part-00246  :  365511\n",
      " 85%|████████▍ | 247/291 [1:23:04<09:52, 13.46s/it]total exampls with hastags:  part-00247  :  362970\n",
      " 85%|████████▌ | 248/291 [1:23:17<09:35, 13.38s/it]total exampls with hastags:  part-00248  :  362721\n",
      " 86%|████████▌ | 249/291 [1:23:31<09:20, 13.35s/it]total exampls with hastags:  part-00249  :  360781\n",
      " 86%|████████▌ | 250/291 [1:23:44<09:04, 13.28s/it]total exampls with hastags:  part-00250  :  361773\n",
      " 86%|████████▋ | 251/291 [1:23:57<08:49, 13.25s/it]total exampls with hastags:  part-00251  :  362096\n",
      " 87%|████████▋ | 252/291 [1:24:10<08:40, 13.34s/it]total exampls with hastags:  part-00252  :  360644\n",
      " 87%|████████▋ | 253/291 [1:24:23<08:23, 13.24s/it]total exampls with hastags:  part-00253  :  358521\n",
      " 87%|████████▋ | 254/291 [1:24:36<08:07, 13.18s/it]total exampls with hastags:  part-00254  :  356929\n",
      " 88%|████████▊ | 255/291 [1:24:49<07:52, 13.13s/it]total exampls with hastags:  part-00255  :  352543\n",
      " 88%|████████▊ | 256/291 [1:25:02<07:36, 13.05s/it]total exampls with hastags:  part-00256  :  352633\n",
      " 88%|████████▊ | 257/291 [1:25:16<07:26, 13.13s/it]total exampls with hastags:  part-00257  :  351745\n",
      " 89%|████████▊ | 258/291 [1:25:28<07:10, 13.04s/it]total exampls with hastags:  part-00258  :  350937\n",
      " 89%|████████▉ | 259/291 [1:25:41<06:54, 12.94s/it]total exampls with hastags:  part-00259  :  351445\n",
      " 89%|████████▉ | 260/291 [1:25:54<06:40, 12.92s/it]total exampls with hastags:  part-00260  :  350652\n",
      " 90%|████████▉ | 261/291 [1:26:07<06:27, 12.91s/it]total exampls with hastags:  part-00261  :  348096\n",
      " 90%|█████████ | 262/291 [1:26:20<06:14, 12.93s/it]total exampls with hastags:  part-00262  :  348398\n",
      " 90%|█████████ | 263/291 [1:26:33<06:02, 12.95s/it]total exampls with hastags:  part-00263  :  346767\n",
      " 91%|█████████ | 264/291 [1:26:46<05:51, 13.00s/it]total exampls with hastags:  part-00264  :  346980\n",
      " 91%|█████████ | 265/291 [1:26:59<05:36, 12.94s/it]total exampls with hastags:  part-00265  :  347528\n",
      " 91%|█████████▏| 266/291 [1:27:13<05:29, 13.18s/it]total exampls with hastags:  part-00266  :  344595\n",
      " 92%|█████████▏| 267/291 [1:27:26<05:16, 13.19s/it]total exampls with hastags:  part-00267  :  344413\n",
      " 92%|█████████▏| 268/291 [1:27:39<05:02, 13.15s/it]total exampls with hastags:  part-00268  :  342477\n",
      " 92%|█████████▏| 269/291 [1:27:52<04:46, 13.02s/it]total exampls with hastags:  part-00269  :  343140\n",
      " 93%|█████████▎| 270/291 [1:28:04<04:30, 12.90s/it]total exampls with hastags:  part-00270  :  341521\n",
      " 93%|█████████▎| 271/291 [1:28:17<04:16, 12.83s/it]total exampls with hastags:  part-00271  :  341261\n",
      " 93%|█████████▎| 272/291 [1:28:29<04:01, 12.72s/it]total exampls with hastags:  part-00272  :  339904\n",
      " 94%|█████████▍| 273/291 [1:28:42<03:47, 12.64s/it]total exampls with hastags:  part-00273  :  339644\n",
      " 94%|█████████▍| 274/291 [1:28:55<03:36, 12.74s/it]total exampls with hastags:  part-00274  :  334859\n",
      " 95%|█████████▍| 275/291 [1:29:07<03:21, 12.62s/it]total exampls with hastags:  part-00275  :  333383\n",
      " 95%|█████████▍| 276/291 [1:29:19<03:06, 12.46s/it]total exampls with hastags:  part-00276  :  332602\n",
      " 95%|█████████▌| 277/291 [1:29:31<02:53, 12.39s/it]total exampls with hastags:  part-00277  :  331174\n",
      " 96%|█████████▌| 278/291 [1:29:43<02:39, 12.29s/it]total exampls with hastags:  part-00278  :  331550\n",
      " 96%|█████████▌| 279/291 [1:29:56<02:26, 12.24s/it]total exampls with hastags:  part-00279  :  329231\n",
      " 96%|█████████▌| 280/291 [1:30:08<02:13, 12.16s/it]total exampls with hastags:  part-00280  :  329470\n",
      " 97%|█████████▋| 281/291 [1:30:19<02:00, 12.05s/it]total exampls with hastags:  part-00281  :  328015\n",
      " 97%|█████████▋| 282/291 [1:30:31<01:48, 12.01s/it]total exampls with hastags:  part-00282  :  325877\n",
      " 97%|█████████▋| 283/291 [1:30:44<01:36, 12.09s/it]total exampls with hastags:  part-00283  :  326037\n",
      " 98%|█████████▊| 284/291 [1:30:55<01:24, 12.02s/it]total exampls with hastags:  part-00284  :  324790\n",
      " 98%|█████████▊| 285/291 [1:31:07<01:11, 11.94s/it]total exampls with hastags:  part-00285  :  324422\n",
      " 98%|█████████▊| 286/291 [1:31:19<00:59, 11.91s/it]total exampls with hastags:  part-00286  :  320769\n",
      " 99%|█████████▊| 287/291 [1:31:31<00:47, 11.86s/it]total exampls with hastags:  part-00287  :  319733\n",
      " 99%|█████████▉| 288/291 [1:31:42<00:35, 11.72s/it]total exampls with hastags:  part-00288  :  316462\n",
      " 99%|█████████▉| 289/291 [1:31:54<00:23, 11.68s/it]total exampls with hastags:  part-00289  :  315944\n",
      "100%|█████████▉| 290/291 [1:32:05<00:11, 11.58s/it]total exampls with hastags:  part-00290  :  306388\n",
      "100%|██████████| 291/291 [1:32:16<00:00, 19.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# 언어별로 hashtag 히스토그램 만들기!\n",
    "file_list = sorted(os.listdir(conf.raw_lzo_path))\n",
    "\n",
    "for file_name in tqdm(file_list):\n",
    "    data_path = conf.raw_lzo_path + file_name\n",
    "    ori_df = read_data(data_path)\n",
    "    df = ori_df[['hashtags', 'tweet_type', 'language', 'tweet_id', 'reply_timestamp', 'retweet_timestamp', 'comment_timestamp', 'like_timestamp']].copy()\n",
    "    df = df.dropna(subset=['hashtags'])\n",
    "    print('total exampls with hastags: ', file_name, ' : ', len(df['hashtags'].dropna()))\n",
    "\n",
    "    df['hashtags_list'] = df['hashtags'].str.split('\\t')\n",
    "    df['hashtags_cnt'] = df['hashtags'].str.count('\\t')\n",
    "    df['hashtags_cnt'] = df['hashtags_cnt'].astype(int) + 1\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id': np.repeat(df.index.to_series(), df['hashtags_cnt']),\n",
    "        'tweet_type': np.repeat(df['tweet_type'], df['hashtags_cnt']),\n",
    "        'language': np.repeat(df['language'], df['hashtags_cnt']),\n",
    "        'retweet_timestamp': np.repeat(df['retweet_timestamp'], df['hashtags_cnt']),\n",
    "        'comment_timestamp': np.repeat(df['comment_timestamp'], df['hashtags_cnt']),\n",
    "        'like_timestamp': np.repeat(df['like_timestamp'], df['hashtags_cnt']),\n",
    "        'reply_timestamp': np.repeat(df['reply_timestamp'], df['hashtags_cnt']),\n",
    "        'hashtags': chainer(df['hashtags']) # row로 나누기\n",
    "    })\n",
    "\n",
    "    # language encode\n",
    "    df['language_encode'] = df['language'].apply(lambda x: language_to_idx[x])\n",
    "\n",
    "    # labeling\n",
    "    for label in (conf.labels):\n",
    "        label_name = label.split('_')[0]\n",
    "        df.loc[df[label]<=0, label_name ] = 0\n",
    "        df.loc[df[label]>0, label_name ] = 1\n",
    "        df = df.drop([label], axis=1)\n",
    "\n",
    "    \n",
    "    aggregate_result = df.groupby(['language_encode', 'hashtags']).count()\n",
    "    # aggregate_result = aggregate_result[[\"id\"]]\n",
    "    aggregate_result['cnt'] = aggregate_result['id'] \n",
    "    aggregate_result = aggregate_result.drop('id', axis=1)\n",
    "    aggregate_result = aggregate_result.reset_index()\n",
    "\n",
    "    top_n = 100\n",
    "    language_hashtags = [[] for _ in range(n_languages)]\n",
    "    for i in range(n_languages):\n",
    "        # try:\n",
    "            tmp = aggregate_result.loc[aggregate_result['language_encode'] == i]\n",
    "            tmp = tmp.sort_values('cnt', ascending=False)[:top_n]\n",
    "\n",
    "            language_hashtags[i] = tmp\n",
    "\n",
    "            data = defaultdict()\n",
    "            with open(f'../data/hashtag/_hashtag_dict{i}.pickle', 'rb') as f:\n",
    "                try:\n",
    "                    data = pickle.load(f)\n",
    "                except:\n",
    "                    # print('no data in pickle')\n",
    "                    pass\n",
    "                    \n",
    "\n",
    "            with open(f'../data/hashtag/_hashtag_dict{i}.pickle', 'wb') as f:\n",
    "                \n",
    "                cnt_data = dict(zip(language_hashtags[i]['hashtags'], language_hashtags[i]['cnt']))\n",
    "                reply_data = dict(zip(language_hashtags[i]['hashtags'], language_hashtags[i]['reply']))\n",
    "                retweet_data = dict(zip(language_hashtags[i]['hashtags'], language_hashtags[i]['retweet']))\n",
    "                comment_data = dict(zip(language_hashtags[i]['hashtags'], language_hashtags[i]['comment']))\n",
    "                like_data = dict(zip(language_hashtags[i]['hashtags'], language_hashtags[i]['like']))\n",
    "\n",
    "                for key, value in cnt_data.items():\n",
    "                    if key in data.keys():\n",
    "                        data[key][0] += value # cnt\n",
    "                        data[key][1] += reply_data[key] # reply\n",
    "                        data[key][2] += retweet_data[key] # retweet\n",
    "                        data[key][3] += comment_data[key] # comment\n",
    "                        data[key][4] += like_data[key] # like\n",
    "                    else:\n",
    "                        data[key] = [value, reply_data[key], retweet_data[key], comment_data[key], like_data[key]]\n",
    "                \n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "                # print(len(data))\n",
    "                # print(data)\n",
    "                del data\n",
    "                del cnt_data\n",
    "\n",
    "        # except:\n",
    "        #     print(f'error')\n",
    "        #     pass\n",
    "    \n",
    "\n",
    "\n",
    "    save_memory(df)\n",
    "    del df\n",
    "    del language_hashtags\n",
    "    del aggregate_result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 147\n1 150\n2 216\n3 225\n4 230\n5 181\n6 183\n7 246\n8 154\n9 351\n10 533\n11 227\n12 1712\n13 340\n14 306\n15 380\n16 499\n17 1177\n18 552\n19 1233\n20 1857\n21 668\n22 638\n23 1308\n24 2706\n25 1218\n26 4736\n27 5037\n28 3710\n29 2004\n30 2844\n31 2923\n32 2090\n33 5056\n34 5169\n35 5248\n36 5117\n37 6274\n38 2256\n39 6144\n40 3346\n41 1296\n42 5849\n43 6131\n44 2923\n45 6307\n46 6125\n47 3927\n48 5372\n49 3743\n50 5005\n51 3032\n52 2375\n53 3408\n54 2352\n55 1715\n56 4253\n57 1572\n58 1695\n59 456\n60 245\n61 107\n62 312\n63 127\n64 41\n65 27\n"
     ]
    }
   ],
   "source": [
    "# load dictionary\n",
    "hashtag_dict = [dict() for _ in range(n_languages)]\n",
    "for i in range(n_languages):\n",
    "    with open(f'../data/hashtag/hashtag_dict{i}.pickle', 'rb') as f:\n",
    "        try:\n",
    "            hashtag_dict[i] = pickle.load(f)\n",
    "            print(i, len(hashtag_dict[i]))\n",
    "        except:\n",
    "            print('no_data')\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1114, 70, 122, 25, 893]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "hashtag_dict[28]['66C64D2A3D611967328A190606C8DCEF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[321, 16, 16, 1, 203], [490, 18, 12, 3, 126], [1114, 70, 122, 25, 893], [350, 37, 21, 9, 69], [145, 3, 3, 0, 33], [24, 0, 0, 0, 24], [13, 0, 0, 0, 1], [56, 1, 0, 0, 27], [28, 2, 0, 0, 11], [11, 0, 0, 0, 1]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([16, 18, 70, 37,  3,  0,  0,  1,  2,  0])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "print(list(hashtag_dict[28].values())[:10])\n",
    "np.array(list(hashtag_dict[28].values())[:10])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target encoding for top_n hashtags with language\n",
    "\n",
    "target_encoder = pd.DataFrame()\n",
    "for i in range(n_languages):\n",
    "    values = np.array(list(hashtag_dict[i].values()))\n",
    "    cnt = values[:,0]\n",
    "    reply = values[:,1]\n",
    "    retweet = values[:,2]\n",
    "    comment = values[:,3]\n",
    "    like = values[:,4]\n",
    "    target_encoder =  pd.concat([target_encoder, pd.DataFrame({'language': np.repeat(idx_to_language[i], len(hashtag_dict[i])), 'hashtags': hashtag_dict[i].keys(), 'cnt': cnt, 'reply': reply, 'retweet': retweet, 'comment': comment, 'like': like})]) \n",
    "\n",
    "target_encoder = target_encoder.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                language                          hashtags  \\\n",
       "0       488B32D24BD4BB44172EB981C1BCA6FA  43B37225C841C6DB6E7D340EAFBA569C   \n",
       "1       488B32D24BD4BB44172EB981C1BCA6FA  A4C4EE3FDED70EFFF280A3C748368825   \n",
       "2       488B32D24BD4BB44172EB981C1BCA6FA  F41EAA37F8AEF033C6664B393B1060B1   \n",
       "3       488B32D24BD4BB44172EB981C1BCA6FA  D9C7E10458181B70421C50A99AB87B78   \n",
       "4       488B32D24BD4BB44172EB981C1BCA6FA  F47F8AD749C074B81AC14ABBE458EAA6   \n",
       "...                                  ...                               ...   \n",
       "147781  8C64085F46CD49FA5C80E72A35845185  3CE892BA90B28C0BA6D7BEBC934C26CD   \n",
       "147782  8C64085F46CD49FA5C80E72A35845185  CBB1D5E37E5E8F4F3EA7A840484C5C64   \n",
       "147783  8C64085F46CD49FA5C80E72A35845185  603B6445385DB6098E70886DEBC9FFB1   \n",
       "147784  8C64085F46CD49FA5C80E72A35845185  6668685BAC46105378AC3B272C4E6977   \n",
       "147785  8C64085F46CD49FA5C80E72A35845185  D9C7E10458181B70421C50A99AB87B78   \n",
       "\n",
       "           cnt  reply  retweet  comment    like  \n",
       "0       671805   4086    66190     5211  210659  \n",
       "1       544271   9093    37163     4015  101807  \n",
       "2       452777   2980    43205     4015  129950  \n",
       "3       404188  11214    21640     2987  135618  \n",
       "4       322776   4909    16528     2315  178016  \n",
       "...        ...    ...      ...      ...     ...  \n",
       "147781       1      0        0        0       1  \n",
       "147782       1      0        0        0       1  \n",
       "147783       1      0        0        0       0  \n",
       "147784       1      0        0        0       0  \n",
       "147785       1      0        0        0       0  \n",
       "\n",
       "[147786 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>language</th>\n      <th>hashtags</th>\n      <th>cnt</th>\n      <th>reply</th>\n      <th>retweet</th>\n      <th>comment</th>\n      <th>like</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>488B32D24BD4BB44172EB981C1BCA6FA</td>\n      <td>43B37225C841C6DB6E7D340EAFBA569C</td>\n      <td>671805</td>\n      <td>4086</td>\n      <td>66190</td>\n      <td>5211</td>\n      <td>210659</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>488B32D24BD4BB44172EB981C1BCA6FA</td>\n      <td>A4C4EE3FDED70EFFF280A3C748368825</td>\n      <td>544271</td>\n      <td>9093</td>\n      <td>37163</td>\n      <td>4015</td>\n      <td>101807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>488B32D24BD4BB44172EB981C1BCA6FA</td>\n      <td>F41EAA37F8AEF033C6664B393B1060B1</td>\n      <td>452777</td>\n      <td>2980</td>\n      <td>43205</td>\n      <td>4015</td>\n      <td>129950</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>488B32D24BD4BB44172EB981C1BCA6FA</td>\n      <td>D9C7E10458181B70421C50A99AB87B78</td>\n      <td>404188</td>\n      <td>11214</td>\n      <td>21640</td>\n      <td>2987</td>\n      <td>135618</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>488B32D24BD4BB44172EB981C1BCA6FA</td>\n      <td>F47F8AD749C074B81AC14ABBE458EAA6</td>\n      <td>322776</td>\n      <td>4909</td>\n      <td>16528</td>\n      <td>2315</td>\n      <td>178016</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>147781</th>\n      <td>8C64085F46CD49FA5C80E72A35845185</td>\n      <td>3CE892BA90B28C0BA6D7BEBC934C26CD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>147782</th>\n      <td>8C64085F46CD49FA5C80E72A35845185</td>\n      <td>CBB1D5E37E5E8F4F3EA7A840484C5C64</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>147783</th>\n      <td>8C64085F46CD49FA5C80E72A35845185</td>\n      <td>603B6445385DB6098E70886DEBC9FFB1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>147784</th>\n      <td>8C64085F46CD49FA5C80E72A35845185</td>\n      <td>6668685BAC46105378AC3B272C4E6977</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>147785</th>\n      <td>8C64085F46CD49FA5C80E72A35845185</td>\n      <td>D9C7E10458181B70421C50A99AB87B78</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>147786 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "target_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MTE_one_shot(folds=5,smooth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding\n",
    "\n",
    "for file_name in tqdm(file_list[:1]):\n",
    "    data_path = conf.raw_lzo_path + file_name\n",
    "    df = read_data(data_path)\n",
    "    df = df.drop('text_tokens', axis=1)    \n",
    "    df = feature_extraction(df, features=conf.used_features + ['hashtags'], train=True) \n",
    "\n",
    "    c = ['language', 'hashtags']\n",
    "    target = 'like'\n",
    "    out_col = 'TE_'+'_'.join(c)+'_'+target\n",
    "\n",
    "    df = encoder.fit_transform(df, c, target, out_col=out_col, out_dtype='float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = conf.raw_lzo_path + 'part-00000'\n",
    "\n",
    "df = read_data(data_path)\n",
    "df = df.groupby(['language', 'hashtags']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}